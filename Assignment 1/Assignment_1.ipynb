{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment 1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tausifshareff/Data-Science/blob/master/Assignment%201/Assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-4fA8NefKE1",
        "colab_type": "text"
      },
      "source": [
        "## Loading all the libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvuQLN8HES50",
        "colab_type": "code",
        "outputId": "5f0b5a3b-cf38-4d91-d3e8-fb282e22b4ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from urllib.request import urlopen\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn import *\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\"\n",
        "%config IPCompleter.greedy=True"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzBhoBLafOR0",
        "colab_type": "text"
      },
      "source": [
        "## Reading the corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCBslw0AFPEl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pos_file = str(urlopen(\"https://raw.githubusercontent.com/fuzhenxin/textstyletransferdata/master/sentiment/pos.txt\").read(), 'utf-8')\n",
        "neg_file = str(urlopen(\"https://raw.githubusercontent.com/fuzhenxin/textstyletransferdata/master/sentiment/neg.txt\").read(), 'utf-8')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSmISHucc2QQ",
        "colab_type": "text"
      },
      "source": [
        "## Removing all the special characters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-BnlM4yf4qP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pos_file = pos_file.splitlines()\n",
        "pos_file = [x.split(' ') for x in pos_file]\n",
        "pos_file = [item for sublist in pos_file for item in sublist]\n",
        "\n",
        "for i in range(0, len(pos_file)):\n",
        "  x = re.search(\"\\.$\", pos_file[i])\n",
        "  if x:\n",
        "    pos_file[i] = pos_file[i].replace(\".\", \"\")\n",
        "  \n",
        "  pos_file[i] = re.sub('[^a-zA-Z0-9\\.]+', '', pos_file[i])\n",
        "\n",
        "pos_file = [str.lower(i) for i in pos_file if i != '']\n",
        "\n",
        "neg_file = neg_file.splitlines()\n",
        "neg_file = [x.split(' ') for x in neg_file]\n",
        "neg_file = [item for sublist in neg_file for item in sublist]\n",
        "\n",
        "for i in range(0, len(neg_file)):\n",
        "  x = re.search(\"\\.$\", neg_file[i])\n",
        "  if x:\n",
        "    neg_file[i] = neg_file[i].replace(\".\", \"\")\n",
        "  \n",
        "  neg_file[i] = re.sub('[^a-zA-Z0-9\\.]+', '', neg_file[i])\n",
        "\n",
        "neg_file = [str.lower(i) for i in neg_file if i != '']\n",
        "\n",
        "# Removing duplicate tokens\n",
        "pos_file = list(dict.fromkeys(pos_file))\n",
        "neg_file = list(dict.fromkeys(neg_file))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NT8Zu6Ivc_JW",
        "colab_type": "text"
      },
      "source": [
        "## Creating two versions of dataset (with and without stopwords)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mlnE_aiYSXyQ",
        "colab_type": "code",
        "outputId": "a301f61f-758c-482a-b9ba-186e43965eef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "print(\"No. of words in pos_file:\", len(pos_file))\n",
        "print(\"No. of words in neg_file:\", len(neg_file))\n",
        "\n",
        "stop_words = stopwords.words('english')\n",
        "\n",
        "pos_without_stop = [x for x in pos_file if x not in stop_words]\n",
        "neg_without_stop = [x for x in neg_file if x not in stop_words]\n",
        "\n",
        "print(\"No. of words in pos_file excluding stopwords:\", len(pos_without_stop))\n",
        "print(\"No. of words in neg_file excluding stopwords:\", len(neg_without_stop))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No. of words in pos_file: 90504\n",
            "No. of words in neg_file: 116655\n",
            "No. of words in pos_file excluding stopwords: 90364\n",
            "No. of words in neg_file excluding stopwords: 116513\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00Nxr3RrdIjd",
        "colab_type": "text"
      },
      "source": [
        "## Splitting dataset into Train, Test and Validation sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbJzEZxMTzpM",
        "colab_type": "code",
        "outputId": "38d5f9e7-10cc-46df-b3d2-40f24acfd940",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "# Train, test, validation splitting\n",
        "\n",
        "split_data_stop = {\n",
        "    \"train\" : np.arange(int(len(pos_file)*0.8)),\n",
        "    \"test\" : np.arange(int(len(pos_file)*0.1)),\n",
        "    \"val\" : np.arange(int(len(pos_file)*0.1))\n",
        "}\n",
        "\n",
        "split_data_stop['train'], split_data_stop['test'] = model_selection.train_test_split(pos_file, test_size = 0.2, random_state = 42)\n",
        "split_data_stop['test'], split_data_stop['val'] = model_selection.train_test_split(split_data_stop['test'], train_size = 0.5, random_state = 42)\n",
        "\n",
        "print(\"Size of Train set with stopwords:\", len(split_data_stop['train']))\n",
        "print(\"Size of Test set with stopwords:\", len(split_data_stop['test']))\n",
        "print(\"Size of Validation set with stopwords\", len(split_data_stop['val']))\n",
        "\n",
        "split_data_nostop = {\n",
        "    \"train\" : np.arange(int(len(pos_without_stop)*0.8)),\n",
        "    \"test\" : np.arange(int(len(pos_without_stop)*0.1)),\n",
        "    \"val\" : np.arange(int(len(pos_without_stop)*0.1))\n",
        "}\n",
        "\n",
        "split_data_nostop['train'], split_data_nostop['test'] = model_selection.train_test_split(pos_without_stop, test_size = 0.2, random_state = 42)\n",
        "split_data_nostop['test'], split_data_nostop['val'] = model_selection.train_test_split(split_data_nostop['test'], train_size = 0.5, random_state = 42)\n",
        "\n",
        "print(\"Size of Train set without stopwords:\", len(split_data_nostop['train']))\n",
        "print(\"Size of Test set without stopwords:\", len(split_data_nostop['test']))\n",
        "print(\"Size of Validation set without stopwords\", len(split_data_nostop['val']))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of Train set with stopwords: 4835715\n",
            "Size of Test set with stopwords: 604464\n",
            "Size of Validation set with stopwords 604465\n",
            "Size of Train set without stopwords: 2455827\n",
            "Size of Test set without stopwords: 306978\n",
            "Size of Validation set without stopwords 306979\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQWwfpM5sCSZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.savetxt()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}